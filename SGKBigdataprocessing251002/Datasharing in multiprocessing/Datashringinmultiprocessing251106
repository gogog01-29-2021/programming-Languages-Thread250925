"""

Single machine
Queue
Pipes

Multiple machine
MPI
File-based communication

Multithreading data flow: directly read/write the same objects; protect with locks. Optional: queue.Queue for clean producer→consumer.

Multiprocessing data flow: nothing is shared unless you choose one: multiprocessing.Queue/Pipe/shared_memory/Array/Value (or files/MPI).

"""

//Queue
import os, math
from multiprocessing import Process

A=[0,0,0,0]

def my_function(i):
	A[i]=A[i]+1
	return

list_of_procs = []
for r in range(4):
	p = Process(target=my_function, args=(r,))  list_of_procs.append(p)
	p.start()
	
#Waits until end of all processes
for p in list_of_procs:
	p.join()

print(A)




//FIFO queue API
from multiprocessing import Process, Queue

queue = Queue()

queue = Queue(maxsize=100)


queue.put(item)

queue.put(item, block=True, timeout=None)


Item = queue.get()


size = queue.qsize()

If queue.empty()



//FIFO Queue
import multiprocessing


def is_even(numbers, q):
    for n in numbers:
        if n % 2 == 0:
            q.put(n)


q = multiprocessing.Queue()
p = multiprocessing.Process(target=is_even, args=(range(20), q))

p.start()
p.join()

while q:
    print q.get()




//pipe
from multiprocessing import Process, Pipe


def f(conn):
    conn.send(['hello world'])
    conn.close()


parent_conn, child_conn = Pipe()
p = Process(target=f, args=(child_conn, ))
p.start()
print parent_conn.recv()
p.join()





////shared memory
# In the first Python interactive shell 
import numpy as np 
a = np.array([1, 1, 2, 3, 5, 8]) 
# Start with an existing NumPy array 
from multiprocessing import shared_memory 
shm = shared_memory.SharedMemory(create=True, size=a.nbytes)
# Now create a NumPy array backed by shared memory 
b = np.ndarray(a.shape, dtype=a.dtype, buffer=shm.buf) 
b[:] = a[:]  //Array int pointer are pointing same content.  Buffer=shm.buf
# Copy the original data into shared memory 
>>> b array([1, 1, 2, 3, 5, 8])

# In either the same shell or a new Python shell on the same machine 
import numpy as np 
from multiprocessing import shared_memory 
# Attach to the existing shared memory block 
existing_shm = shared_memory.SharedMemory(name='psm_21467_46075’) 
# Note that a.shape is (6,) and a.dtype is np.int64 in this example 
c = np.ndarray((6,), dtype=np.int64, buffer=existing_shm.buf) 
c 
>>> array([1, 1, 2, 3, 5, 8]) 
c[-1] = 888 
>>> c array([ 1, 1, 2, 3, 5, 888]) 



//FIle based sharing
import numpy as np 
a = np.array([1, 1, 2, 3, 5, 8]) 

#Using numpy library
np.savetxt('test.out’, a, delimiter=',’) 

#Using OS version
with open("test2.txt", "wb") as fid: 
   a = np.array([[1, 2], [3, 4]]) 
   fid.write(a.astype("float64"))




//MPI
brew install openmpi
pip install mpi4py
pip install ipyparallel
import ipyparallel as ipp

rc = ipp.Cluster(engines='mpi', n=4).start_and_connect_sync()
rc.wait_for_engines(4)

view = rc[:]
view.activate()


%%px --block
# MPI initialization, library imports and sanity checks on all engines
from mpi4py import MPI

mpi = MPI.COMM_WORLD
print("MPI rank: %i/%i" % (mpi.rank,mpi.size))


//send & Receive
%%px --block
# MPI initialization, library imports and sanity checks on all engines
from mpi4py import MPI
import numpy

comm = MPI.COMM_WORLD 
rank = comm.Get_rank() 

if rank == 0: 
    data = numpy.arange(100, dtype=numpy.float64) 
    print(data)
    comm.Send(data, dest=1, tag=13) 
elif rank == 1: 
    data = numpy.empty(100, dtype=numpy.float64)
    comm.Recv(data, source=0, tag=13)
    print(data)





